{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BVEc56zl9hj"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PaLM on top of Vertex AI Search"
      ],
      "metadata": {
        "id": "Qe6LXTPlmlT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search API"
      ],
      "metadata": {
        "id": "hsp33-22-GwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Search API\n",
        "\n",
        "# ! pip install google-cloud-discoveryengine==0.11.0 --quiet\n",
        "from google.cloud import discoveryengine_v1beta as discoveryengine\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from typing import List, Dict\n",
        "\n",
        "def search_unstructured(PROJECT_ID, data):\n",
        "    engine_id = data.get('engine_id')\n",
        "    page_token = data.get('page_token')\n",
        "    query = data.get('search_query')\n",
        "    page_size = data.get('page_size')\n",
        "    summary_result_count = data.get('summary_result_count')\n",
        "    max_snippet_count = data.get('max_snippet_count')\n",
        "    max_extractive_answer_count = data.get('max_extractive_answer_count')\n",
        "    max_extractive_segment_count = data.get('max_extractive_segment_count')\n",
        "    filter_id = data.get('filter_id')\n",
        "    filter_facets = data.get('filter_facets')\n",
        "    filter_tenant = data.get('filter_tenant')\n",
        "\n",
        "    return search(\n",
        "      PROJECT_ID,\n",
        "      engine_id,\n",
        "      page_token,\n",
        "      query,\n",
        "      page_size,\n",
        "      summary_result_count,\n",
        "      max_snippet_count,\n",
        "      max_extractive_answer_count,\n",
        "      max_extractive_segment_count,\n",
        "      filter_id,\n",
        "      filter_facets,\n",
        "      filter_tenant\n",
        "    )\n",
        "\n",
        "def search(\n",
        "    project_id,\n",
        "    search_engine_id,\n",
        "    page_token: str,\n",
        "    search_query: str,\n",
        "    page_size: int,\n",
        "    summary_result_count: int,\n",
        "    max_snippet_count: int,\n",
        "    max_extractive_answer_count: int,\n",
        "    max_extractive_segment_count: int,\n",
        "    filter_id=List[str],\n",
        "    filter_facets=Dict[str, List[str]],\n",
        "    filter_tenant=str,\n",
        "\n",
        "    facets=[\"category\",\"tenant\"]\n",
        "    ):\n",
        "\n",
        "    ## Create a client\n",
        "    client = discoveryengine.SearchServiceClient()\n",
        "\n",
        "    ## Argolis Parameters\n",
        "    project_id = project_id\n",
        "    search_engine_id = search_engine_id\n",
        "    location = \"global\"\n",
        "    serving_config_id = \"default_config\"\n",
        "    search_query = search_query\n",
        "\n",
        "    ## Config Path\n",
        "    serving_config = client.serving_config_path(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        data_store=search_engine_id,\n",
        "        serving_config=serving_config_id,\n",
        "    )\n",
        "\n",
        "    ## Content Search Spec\n",
        "    content_search_spec = {\n",
        "        'snippet_spec': { 'return_snippet': True },\n",
        "        'extractive_content_spec': {\n",
        "            'max_extractive_answer_count': max_extractive_answer_count,\n",
        "            'max_extractive_segment_count' : max_extractive_segment_count\n",
        "        },\n",
        "        'summary_spec': {\n",
        "            'summary_result_count' : summary_result_count,\n",
        "            'include_citations': True,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    ## Facets\n",
        "    facet_specs=[] ## if empty, no facets are returned (max=100)\n",
        "    for facet in facets:\n",
        "        facet_spec = {\n",
        "            \"facet_key\": {\n",
        "                \"key\": facet,\n",
        "            },\n",
        "            \"limit\": 20, ## Default=20, max facet values to be returned for this facet (max=300)\n",
        "        }\n",
        "        facet_specs.append(facet_spec)\n",
        "\n",
        "    filter_parts = []\n",
        "    if filter_id:\n",
        "        filter_id_string = \"id: ANY(\\\"\" + \"\\\", \\\"\".join(filter_id) + \"\\\")\"\n",
        "        filter_parts.append(filter_id_string)\n",
        "    if filter_facets:\n",
        "        category_filters = filter_facets.get('category', [])\n",
        "        if category_filters:\n",
        "            category_filter_string = \"category: ANY(\\\"\" + \"\\\", \\\"\".join(category_filters) + \"\\\")\"\n",
        "            filter_parts.append(category_filter_string)\n",
        "        tenant_filters = filter_facets.get('tenant', [])\n",
        "        if tenant_filters:\n",
        "            tenant_filter_string = \"tenant: ANY(\\\"\" + \"\\\", \\\"\".join(tenant_filters) + \"\\\")\"\n",
        "            filter_parts.append(tenant_filter_string)\n",
        "    if filter_tenant:\n",
        "        filter_tenant_string = \"tenant: ANY(\\\"\" + filter_tenant + \"\\\")\"\n",
        "        filter_parts.append(filter_tenant_string)\n",
        "    filter_string = \" AND \".join(filter_parts)\n",
        "\n",
        "    ## Request\n",
        "    request = discoveryengine.SearchRequest(\n",
        "        serving_config=serving_config,\n",
        "        page_token=page_token,\n",
        "        query=search_query,\n",
        "        page_size=page_size,\n",
        "        content_search_spec=content_search_spec,\n",
        "        facet_specs=facet_specs,\n",
        "        filter=filter_string,\n",
        "    )\n",
        "\n",
        "    response = client.search(request)\n",
        "    # print(\"~ API Response:\\n\",response)\n",
        "\n",
        "    formatted_results = []\n",
        "    results = response.results\n",
        "    numOfResults = len(results)\n",
        "    totalSize = response.total_size\n",
        "    attributionToken = response.attribution_token\n",
        "    nextPageToken = response.next_page_token\n",
        "    summary = response.summary.summary_text\n",
        "    corrected_query = response.corrected_query\n",
        "    facets = response.facets\n",
        "    facetDict = {}\n",
        "    for facet in facets:\n",
        "        key = facet.key\n",
        "        values_dict = {value.value: value.count for value in facet.values}\n",
        "        facetDict[key] = values_dict\n",
        "\n",
        "    some_results = {\n",
        "      \"numOfResults\": numOfResults,\n",
        "      \"totalSize\": totalSize,\n",
        "      \"token\": attributionToken,\n",
        "      \"nextPageToken\": nextPageToken,\n",
        "      \"summary\": summary,\n",
        "      \"corrected_query\": corrected_query,\n",
        "      \"facets\": facetDict\n",
        "    }\n",
        "\n",
        "    formatted_results.append(some_results)\n",
        "\n",
        "    for result in response.results:\n",
        "      data = MessageToDict(result.document._pb)\n",
        "      formatted_result = {}\n",
        "      formatted_result['id'] = data.get('id', {})\n",
        "      formatted_result['filter_category'] = data.get('structData', {}).get('category',{})\n",
        "      formatted_result['filter_id'] = data.get('structData', {}).get('id',{})\n",
        "      formatted_result['filter_name'] = data.get('structData', {}).get('name',{})\n",
        "      formatted_result['filter_tenant'] = data.get('structData', {}).get('tenant',{})\n",
        "      formatted_result['filter_summary'] = data.get('structData', {}).get('summary',{})\n",
        "      formatted_result['filter_num_pages'] = data.get('structData', {}).get('num_pages',{})\n",
        "      formatted_result['filter_created_time'] = data.get('structData', {}).get('created_time',{})\n",
        "\n",
        "      formatted_result['snippets'] = [d.get('snippet') for d in data.get('derivedStructData', {}).get('snippets', []) if d.get('snippet') is not None]\n",
        "      formatted_result['pageNumber'] = [d.get('pageNumber') for d in data.get('derivedStructData', {}).get('snippets', []) if d.get('pageNumber') is not None]\n",
        "      formatted_result['uri_link'] = data.get('derivedStructData', {}).get('link',{})\n",
        "      formatted_result['extractive_answers_content'] = [d.get('content') for d in data.get('derivedStructData', {}).get('extractive_answers', []) if d.get('content') is not None]\n",
        "      formatted_result['extractive_answers_pageNumber'] = [d.get('pageNumber') for d in data.get('derivedStructData', {}).get('extractive_answers', []) if d.get('pageNumber') is not None]\n",
        "      formatted_result['extractive_segments'] = [d.get('content') for d in data.get('derivedStructData', {}).get('extractive_segments', []) if d.get('content') is not None]\n",
        "\n",
        "      formatted_results.append(formatted_result)\n",
        "\n",
        "    # print(\"~ Formatted Results:\\n\",formatted_results)\n",
        "    # return response, formatted_results\n",
        "    return formatted_results\n"
      ],
      "metadata": {
        "id": "TV-TSNY_m2ko",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Search\n",
        "\n",
        "project_id = \"elroy-demo\" #@param {type: \"string\"}\n",
        "engine_id = \"pca_1693716174825\" #@param {type: \"string\"}\n",
        "page_token = \"\"\n",
        "search_query = \"cloudsql vs bigquery\" #@param {type: \"string\"}\n",
        "page_size = 3 #@param {type: \"integer\"}\n",
        "summary_result_count = 1 #@param {type: \"integer\"}\n",
        "max_snippet_count = 1 #@param {type: \"integer\"}\n",
        "max_extractive_answer_count = 2 #@param {type: \"integer\"}\n",
        "max_extractive_segment_count = 2 #@param {type: \"integer\"}\n",
        "filter_id = []\n",
        "filter_facets = []\n",
        "filter_tenant = \"\"\n",
        "\n",
        "request = {\n",
        "    \"project_id\": project_id,\n",
        "    \"engine_id\": engine_id,\n",
        "    \"page_token\": page_token,\n",
        "    \"search_query\": search_query,\n",
        "    \"page_size\": page_size,\n",
        "    \"summary_result_count\": summary_result_count,\n",
        "    \"max_snippet_count\": max_snippet_count,\n",
        "    \"max_extractive_answer_count\": max_extractive_answer_count,\n",
        "    \"max_extractive_segment_count\": max_extractive_segment_count,\n",
        "    \"filter_id\": filter_id,\n",
        "    \"filter_facets\": filter_facets,\n",
        "    \"filter_tenant\": filter_tenant\n",
        "}\n",
        "\n",
        "response = search(\n",
        "    request[\"project_id\"],\n",
        "    request[\"engine_id\"],\n",
        "    request[\"page_token\"],\n",
        "    request[\"search_query\"],\n",
        "    request[\"page_size\"],\n",
        "    request[\"summary_result_count\"],\n",
        "    request[\"max_snippet_count\"],\n",
        "    request[\"max_extractive_answer_count\"],\n",
        "    request[\"max_extractive_segment_count\"],\n",
        "    request[\"filter_id\"],\n",
        "    request[\"filter_facets\"],\n",
        "    request[\"filter_tenant\"],\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "WYn03tP0nOGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa96a2e-3dad-4daa-9574-dc3a8f625916",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'numOfResults': 3,\n",
              "  'totalSize': 3847,\n",
              "  'token': 'W_BaCgwIgYbeqQYQ_qbTzwISJDY1MzJlZDk5LTAwMDAtMmM1MS05YzZjLTE0MjIzYmFiODkyYSIHR0VORVJJQyoc1LKdFZyGjiKiho4ijr6dFaaL7xfFy_MXwvCeFQ',\n",
              "  'nextPageToken': 'QYykDOiFmYzIjM0ETLjZzY50SM1MmMtADMwATL4kDZlJzM1YDJaIA0hv9_QYQqt_egIwgEzEgC',\n",
              "  'summary': 'BigQuery is a good option for meeting the requirement to store at least 10 TB of historical data [1].',\n",
              "  'corrected_query': '',\n",
              "  'facets': {'category': {'pca': 7843}, 'tenant': {'gcp': 7843}}},\n",
              " {'id': 'Official-Google-Cloud-Certified-Professi_20230827171845',\n",
              "  'filter_category': 'pca',\n",
              "  'filter_id': 'Official-Google-Cloud-Certified-Professi_20230827171845',\n",
              "  'filter_name': 'Official Google Cloud Certified Professional Cloud Architect Study Guide',\n",
              "  'filter_tenant': 'gcp',\n",
              "  'filter_summary': {},\n",
              "  'filter_num_pages': {},\n",
              "  'filter_created_time': {},\n",
              "  'snippets': ['Designing for Technical Requirements Fully managed and serverless databases, such as Cloud Datastore and <b>BigQuery</b> ... <b>Cloud SQL</b> and Bigtable , can be made more&nbsp;...'],\n",
              "  'pageNumber': [],\n",
              "  'uri_link': 'gs://pca-pdf/docs/Official Google Cloud Certified Professional Cloud Architect Study Guide.pdf',\n",
              "  'extractive_answers_content': ['BigQuery is also a good option for meeting the requirement to store at least 10 TB of historical data. (Currently, Cloud SQL can store up to 10 TB, which makes BigQuery a good choice).',\n",
              "   'The BigQuery Data Transfer Service is a specialized service for loading data from other cloud services, such as Google Ads and Google Ad Managers. It also supports transferring data from Cloud Storage and AWS S3, but these are both are in beta stage at the time of this writing.'],\n",
              "  'extractive_answers_pageNumber': ['94', '139'],\n",
              "  'extractive_segments': ['Databases\\n\\n101\\n\\nBigQuery is integrated with Cloud IAM, which has several predefined roles for\\nBigQuery.\\ndataViewer. This role allows a user to list projects and tables and get table data and\\nmetadata.\\ndataEditor. This has the same permissions as dataViewer, plus permissions to create\\nand modify tables and datasets.\\ndataOwner. This role is similar to dataEditor, but it can also create, modify, and delete\\ndatasets.\\nmetadataViewer. This role gives permissions to list tables, projects, and datasets.\\nuser. The user role gives permissions to list projects and tables, view metadata, create\\ndatasets, and create jobs.\\njobUser. A jobUser can list projects and create jobs and queries.\\nadmin. An admin can perform all operations on BigQuery resources.\\nBigQuery is billed based on the amount of data stored and the amount of data scanned\\nwhen responding to queries, or in the case of flat-rate query billing, the allocation is used\\nbased on the size of the query. For this reason, it is best to craft queries that return only the\\ndata that is needed, and filter criteria should be as specific as possible.\\nIf you are interested in viewing the structure of a table or view or you want to see sam\\nple data, it is best to use the Preview Option in the console or use the bq head command\\nfrom the command line. BigQuery also provides a --dry-run option for command-line que\\nries. It returns an estimate of the number of bytes that would be returned if the query were\\nexecuted.\\nThe BigQuery Data Transfer Service is a specialized service for loading data from other\\ncloud services, such as Google Ads and Google Ad Managers. It also supports transferring\\ndata from Cloud Storage and AWS S3, but these are both are in beta stage at the time of\\nthis writing.\\nGCP provides two managed relational databases and an analytics database with some\\nrelational features. Cloud SQL is used for transaction processing systems that do not need\\nto scale beyond a single server. It supports MySQL and PostgreSQL. Cloud Spanner is a\\ntransaction processing relational database that scales horizontally, and it is used when a\\nsingle server relational database is insufficient. BigQuery is designed for data warehousing\\nand analytic querying of large datasets. BigQuery should not be used for transaction pro\\ncessing systems. If data is frequently updated after loading, then one of the other managed\\nrelational databases is a better option.',\n",
              "   'NoSQL Databases\\nGCP offers three NoSQL databases: Bigtable, Datastore, and Cloud Firestore. All three are\\nwell suited to storing data that requires flexible schemas. Cloud Bigtable is a wide-column\\nNoSQL database. Cloud Firestore and Cloud Datastore are document NoSQL databases.\\nCloud Firebase is the next generation of Cloud Datastore.']},\n",
              " {'id': 'Using-Google-Cloud-Managed-Storage-Servi_20230827171845',\n",
              "  'filter_category': 'pca',\n",
              "  'filter_id': 'Using-Google-Cloud-Managed-Storage-Servi_20230827171845',\n",
              "  'filter_name': '10  Using Google Cloud Managed Storage Services from Kubernetes Applications ILT v1.7',\n",
              "  'filter_tenant': 'gcp',\n",
              "  'filter_summary': {},\n",
              "  'filter_num_pages': {},\n",
              "  'filter_created_time': {},\n",
              "  'snippets': ['Enable the APIs Firestore <b>Cloud SQL BigQuery</b> Cloud Bigtable Cloud Spanner Role Transaction processing NoSQL SQL Analytics processing You can use the same&nbsp;...'],\n",
              "  'pageNumber': [],\n",
              "  'uri_link': 'gs://pca-pdf/docs/10_ Using Google Cloud Managed Storage Services from Kubernetes Applications_ILT v1.7.pdf',\n",
              "  'extractive_answers_content': ['Enable the APIs Firestore Cloud SQL BigQuery Cloud Bigtable Cloud Spanner Role Transaction processing NoSQL SQL Analytics processing You can use the same approach that you use to access Cloud Storage for accessing other Google Cloud services, such as Cloud Bigtable, BigQuery, Firestore, Cloud SQL, and Cloud Spanner.',\n",
              "   'Now I&#39;ll tell you about Google Cloud&#39;s data warehousing service. Even though BigQuery supports SQL queries, it&#39;s different from Google Cloud&#39;s other relational database services in that it&#39;s a columnar store.'],\n",
              "  'extractive_answers_pageNumber': ['20', '29'],\n",
              "  'extractive_segments': ['Using Google Cloud Services\\nUsing Cloud Storage\\nUsing Google Cloud Databases\\nLab: Using Cloud SQL with Google\\nKubernetes Engine\\nSummary\\n\\nAgenda\\n\\nNow let’s look at the types of managed database services provided by Google Cloud.\\nIn this lesson we will look at the key features of Cloud Bigtable, BigQuery, Firestore,\\nCloud SQL and Cloud Spanner. You will also learn how to use Cloud SQL Proxy to\\nsimplify the process of connecting GKE applications to Cloud SQL instances.',\n",
              "   'Enable the APIs\\n\\nFirestore\\n\\nCloud\\nSQL\\n\\nBigQuery\\n\\nCloud\\nBigtable\\n\\nCloud\\nSpanner\\n\\nRole\\n\\nTransaction processing\\n\\nNoSQL\\n\\nSQL\\n\\nAnalytics processing\\n\\nYou can use the same approach that you use to access Cloud Storage for accessing\\nother Google Cloud services, such as Cloud Bigtable, BigQuery, Firestore, Cloud\\nSQL, and Cloud Spanner.\\nGoogle Cloud’s database services serve two overall purposes. Many are suitable for\\nuse to support an application’s online data use, such as its storage and retrieval of\\ndata. On the other hand, Google Cloud also offers an analytics database service\\nthat’s optimized for data mining and discovery of patterns and trends.\\n\\nIn turn, the database services that back online applications fall into the categories of\\nSQL and NoSQL. SQL database services are for relational data, and you choose\\nthem when you want the database engine to help you enforce the semantics of the\\ndatabase. NoSQL database services are for flexibly structured data, and it’s up to\\nyour application to maintain your data’s integrity.\\nYou’ll learn how to choose among these database services later in this lesson. But,\\nwhichever you choose, make sure that you enable the API of the service and give\\nyour applications appropriate Cloud IAM service accounts. You will do this in the lab\\nfor Cloud SQL.\\n\\nNow I’ll introduce you to each of the Google Cloud database services. You can use']},\n",
              " {'id': 'Prep-for-PCA---Designing-and-Implementin_20230827171845',\n",
              "  'filter_category': 'pca',\n",
              "  'filter_id': 'Prep-for-PCA---Designing-and-Implementin_20230827171845',\n",
              "  'filter_name': '03 Prep for PCA   Designing and Implementing v1.2',\n",
              "  'filter_tenant': 'gcp',\n",
              "  'filter_summary': {},\n",
              "  'filter_num_pages': {},\n",
              "  'filter_created_time': {},\n",
              "  'snippets': ['Datastore Bigtable Cloud Storage <b>Cloud SQL</b> Cloud Spanner <b>BigQuery</b> Type NoSQL document NoSQL wide column Blobstore Relational SQL for OLTP Relational SQL for&nbsp;...'],\n",
              "  'pageNumber': [],\n",
              "  'uri_link': 'gs://pca-pdf/docs/03 Prep for PCA - Designing and Implementing v1.2.pdf',\n",
              "  'extractive_answers_content': ['● If you need to store immutable blobs larger than 10 MB, such as large images or movies, consider Cloud Storage. ● If you need to store highly structured objects, or if you require support for ACID transactions and SQL-like queries, consider Datastore. ● If you need interactive querying in an online analytical processing (OLAP) system, consider BigQuery.',\n",
              "   'It is a &quot;serverless&quot; service, meaning that it is fully managed. So users do not have visibility or control over individual servers or clusters of servers. BigQuery runs data processing jobs that can load, export, copy or query data.'],\n",
              "  'extractive_answers_pageNumber': ['46', '88'],\n",
              "  'extractive_segments': ['Datastore\\n\\nBigtable\\n\\nCloud Storage\\n\\nCloud SQL Cloud Spanner BigQuery\\n\\nType\\n\\nNoSQL\\ndocument\\n\\nNoSQL\\nwide column\\n\\nBlobstore\\n\\nRelational\\nSQL for OLTP\\n\\nRelational\\nSQL for OLTP\\n\\nRelational\\nSQL for OLAP\\n\\nTransactions Yes\\n\\nSingle-row\\n\\nNo\\n\\nYes\\n\\nYes\\n\\nNo\\n\\nComplex\\nqueries\\n\\nYes\\n\\nNo\\n\\nNo\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nCapacity\\n\\nTerabytes+\\n\\nPetabytes+\\n\\nPetabytes+\\n\\n10,230 GB\\n\\nPetabytes\\n\\nPetabytes+\\n\\nUnit size\\n\\n1 MB/entity\\n\\n~10 MB/cell\\n~100\\nMB/row\\n\\n5 TB/object\\n\\nDetermined\\nby DB engine\\n10,240 MiB/\\n\\nrow\\n\\n10 MB/row\\n\\nComparing storage options: Technical details\\n\\n● Cloud Bigtable is not a relational database; it does not support SQL queries or\\njoins, nor does it support multi-row transactions. Also, it is not a good solution\\nfor small amounts of data (< 1 TB).\\n\\n●\\n\\nIf you need full SQL support for an online transaction processing (OLTP)\\nsystem, consider Cloud SQL and Cloud Spanner. Cloud Spanner is\\nparticularly suited for databases larger than about 2 TB and databases that will\\nbe written to by global clients.\\n\\n●\\n\\nIf you need to store immutable blobs larger than 10 MB, such as large images\\nor movies, consider Cloud Storage.\\n\\n●\\n\\nIf you need to store highly structured objects, or if you require support for\\nACID transactions and SQL-like queries, consider Datastore.\\n\\n●\\n\\nIf you need interactive querying in an online analytical processing (OLAP)\\nsystem, consider BigQuery.',\n",
              "   'BigQuery is a cloud data warehouse service\\n\\nColossus\\n\\nBigQuery Storage Service\\nProject\\nDataset A\\n\\nTable\\n1\\n\\nTable\\n2\\n\\nTable\\n3\\n\\nDataset B\\n\\nTable\\n1\\n\\nTable\\n2\\n\\nBigQuery Query Service\\n\\nPetabit\\nnetwork\\n\\nWeb UI\\nbq\\nREST API\\n7-languages\\nsupported\\n\\nCSV files in\\nCloud Storage\\nConnectors to\\nother services\\n\\nBulk data ingest\\nStreaming\\ningest\\n\\nBigQuery is a data warehouse service. It is a \"serverless\" service, meaning that it is\\nfully managed. So users do not have visibility or control over individual servers or\\nclusters of servers. BigQuery runs data processing jobs that can load, export, copy or\\nquery data.\\nBigQuery has two parts, a storage service and a query service, which work together.\\nThey are connected by Google\\'s high speed internal network.\\nThe storage service manages the data. Data is contained within a project in datasets\\nin tables. The tables are stored as highly compressed columns in Google’s Colossus\\nfile system which provides durability and availability.\\nBigQuery Storage Service automatically shards and shuffles data in the underlying file\\nsystem to provide a very high level of service at huge scales. The sharding occurs\\nautomatically and provides the advantages of data distribution while completely\\nconcealed from you at the Dataset and Table level.\\nThe storage service supports bulk data ingest and streaming ingest. So it can work\\nwith huge amounts of data and also real-time data streams.\\nThe query service runs interactive or batch queries that are submitted through\\nconsole, the BigQuery web UI, the bq command line tool, or via REST API. The REST\\nAPI is supported for seven programming languages.']}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertex AI"
      ],
      "metadata": {
        "id": "arSEfU6J-J0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VertexAI\n",
        "\n",
        "! pip install google-cloud-aiplatform --quiet\n",
        "\n",
        "import re\n",
        "import os\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def vertex_qa(\n",
        "  project_id,\n",
        "  llmModel,\n",
        "  query,\n",
        "  summary,\n",
        "  searchResults,\n",
        "  userInput,\n",
        "  temperature,\n",
        "  topK,\n",
        "  topP\n",
        "  ):\n",
        "\n",
        "  ######################################################################################################\n",
        "\n",
        "  \"\"\" Snippet / Answer / Segment \"\"\"\n",
        "  snippet_pattern = r\"\\{\\{snippet_(\\d+)\\}\\}\"\n",
        "  answer_pattern = r\"\\{\\{answer_(\\d+)-(\\d+)\\}\\}\"\n",
        "  segment_pattern = r\"\\{\\{segment_(\\d+)-(\\d+)\\}\\}\"\n",
        "  doc_pattern = r\"\\{\\{docName_(\\d+)\\}\\}\"\n",
        "\n",
        "  # Find all occurrences of each item in the userInput\n",
        "  snippets = re.findall(snippet_pattern, userInput)\n",
        "  answers = re.findall(answer_pattern, userInput)\n",
        "  segments = re.findall(segment_pattern, userInput)\n",
        "  docs = re.findall(doc_pattern, userInput)\n",
        "\n",
        "  # Create a dictionary to store the key-value pairs\n",
        "  result_dict = {}\n",
        "\n",
        "  # Process snippet items and store in the result_dict\n",
        "  for snippet_index in snippets:\n",
        "    snippet_index = int(snippet_index)\n",
        "    if snippet_index <= len(searchResults):\n",
        "      # print(\"Snippet:\",snippet_index)\n",
        "      value = searchResults[snippet_index - 1][\"snippets\"][0]\n",
        "    else:\n",
        "      value = \"\"\n",
        "    key = f\"{{snippet_{snippet_index}}}\"\n",
        "    result_dict[key] = value\n",
        "\n",
        "  # Process answer items and store in the result_dict\n",
        "  for answer_index_1, answer_index_2 in answers:\n",
        "    answer_index_1, answer_index_2 = int(answer_index_1), int(answer_index_2)\n",
        "    if answer_index_1 <= len(searchResults) and answer_index_2 <= len(searchResults[answer_index_1 - 1][\"extractive_answers_content\"]):\n",
        "      # print(\"Answer:\", answer_index_1, answer_index_2)\n",
        "      value = searchResults[answer_index_1 - 1][\"extractive_answers_content\"][answer_index_2 - 1]\n",
        "    else:\n",
        "      value = \"\"\n",
        "    key = f\"{{answer_{answer_index_1}-{answer_index_2}}}\"\n",
        "    result_dict[key] = value\n",
        "\n",
        "  # Process answer items and store in the result_dict\n",
        "  for segment_index_1, segment_index_2 in segments:\n",
        "    segment_index_1, segment_index_2 = int(segment_index_1), int(segment_index_2)\n",
        "    if segment_index_1 <= len(searchResults) and segment_index_2 <= len(searchResults[segment_index_1 - 1][\"extractive_segments\"]):\n",
        "      # print(\"Segment:\", segment_index_1, segment_index_2)\n",
        "      value = searchResults[segment_index_1 - 1][\"extractive_segments\"][segment_index_2 - 1]\n",
        "    else:\n",
        "      value = \"\"\n",
        "    key = f\"{{segment_{segment_index_1}-{segment_index_2}}}\"\n",
        "    result_dict[key] = value\n",
        "\n",
        "  # Process doc items and store in the result_dict\n",
        "  for doc_index in docs:\n",
        "    doc_index = int(doc_index)\n",
        "    if doc_index <= len(searchResults):\n",
        "      # print(\"Doc:\", doc_index)\n",
        "      value = searchResults[doc_index - 1][\"filter_name\"]\n",
        "    else:\n",
        "      value = \"\"\n",
        "    key = f\"{{docName_{doc_index}}}\"\n",
        "    result_dict[key] = value\n",
        "\n",
        "  # Process other {{}} and store in the result_dict\n",
        "  result_dict[\"{{query}}\"] = query\n",
        "  result_dict[\"{{summary}}\"] = summary\n",
        "\n",
        "  ######################################################################################################\n",
        "\n",
        "  \"\"\" Vertex AI \"\"\"\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models\n",
        "  max_output_tokens = 1024 # text-bison\n",
        "  if llmModel==\"text-bison-32k\":\n",
        "    max_output_tokens = 8192 # max output for text-bison-32k\n",
        "\n",
        "  temperature = 0.0 if temperature<0.0 else temperature\n",
        "  temperature = 1.0 if temperature>1.0 else temperature\n",
        "  topK = 1 if topK<1 else topK\n",
        "  topK = 40 if topK>40 else topK\n",
        "  topP = 0.0 if topP<0.0 else topP\n",
        "  topP = 1.0 if topP>1.0 else topP\n",
        "\n",
        "  # llm = VertexAI(\n",
        "  #   model_name=llmModel,\n",
        "  #   max_output_tokens=max_output_tokens,\n",
        "  #   temperature=temperature,\n",
        "  #   top_k=topK,\n",
        "  #   top_p=topP,\n",
        "  #   verbose=True)\n",
        "\n",
        "  ## Replace each {x} with its actual value\n",
        "  for i in result_dict:\n",
        "    userInput = userInput.replace(i, result_dict[i])\n",
        "\n",
        "  # result = llm(userInput)\n",
        "  result = predict_large_language_model_sample(project_id, llmModel, temperature, topP, topK, userInput)\n",
        "\n",
        "\n",
        "  # print(\"Final Prompt:\\n\", userInput)\n",
        "  # print(\"-----------\")\n",
        "  print(\"LLM Output:\\n\", result)\n",
        "\n",
        "  return result\n",
        "\n",
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    content: str,\n",
        "    max_decode_steps: int=256,\n",
        "    location: str = \"us-central1\",\n",
        "    tuned_model_name: str = \"\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(model_name)\n",
        "    if tuned_model_name:\n",
        "      model = model.get_tuned_model(tuned_model_name)\n",
        "    response = model.predict(\n",
        "        content,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_decode_steps,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,)\n",
        "\n",
        "    # print(f\"Response from Model: {response.text}\")\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "dHV5-jHMmouy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run VertexAI\n",
        "\n",
        "project_id = \"elroy-demo\" #@param {type: \"string\"}\n",
        "llmModel = \"text-bison\" #@param {type: \"string\"}\n",
        "query = search_query\n",
        "summary = response[0]['summary']\n",
        "searchResults = response[1:]\n",
        "prompt = \"Using only the given context, provide a summary answer for query and structure your answer in a markdown table format.\" #@param {type: \"string\"}\n",
        "temperature = 0\n",
        "topK = 40\n",
        "topP = 0.95\n",
        "\n",
        "ans = vertex_qa(\n",
        "  project_id,\n",
        "  llmModel,\n",
        "  query,\n",
        "  summary,\n",
        "  searchResults,\n",
        "  prompt,\n",
        "  temperature,\n",
        "  topK,\n",
        "  topP\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zszeg8Q4oehR",
        "outputId": "1533252a-d2f3-40f4-9ebb-ba7838eb208b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output:\n",
            "  | Question | Answer |\n",
            "| ----------- | ----------- |\n",
            "| What is the main idea of the passage? | The main idea of the passage is that using only the given context, one can provide a summary answer for a query and structure the answer in a markdown table format. |\n",
            "| What are the steps involved in providing a summary answer for a query? | The steps involved in providing a summary answer for a query are: \n",
            "1. Identify the main idea of the passage. \n",
            "2. Structure the answer in a markdown table format. \n",
            "3. Include only the given context in the answer. |\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}